{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMEN RECUPERACION DE INFORMACION\n",
    "\n",
    "Nombre: Sergio Guaman\n",
    "Curso: GR1-CC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 1: Selecci√≥n y Preprocesamiento del Corpus (4 puntos)\n",
    "\n",
    "Se trabajar√° con el corpus 20 Newsgroups, un conjunto de documentos de texto extra√≠dos de foros de discusi√≥n en diversas categor√≠as. Se puede descargar con sklearn.datasets.fetch_20newsgroups.\n",
    "\n",
    "    Carga del corpus (1 punto): Descargar y visualizar ejemplos de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categor√≠as disponibles: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Documento 1:\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a...\n",
      "\n",
      "\n",
      "Documento 2:\n",
      "My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "...\n",
      "\n",
      "\n",
      "Documento 3:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
      "\tOr have you changed your calendar???\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t    ****************\n",
      "\t\t\t\t\t\t    ******************\n",
      "\t\t\t    ***************\n",
      "\n",
      "\n",
      "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
      "\t\n",
      "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
      "\t\t\t\t\t\t    ***...\n",
      "\n",
      "\n",
      "Documento 4:\n",
      "\n",
      "Think!\n",
      "\n",
      "It's the SCSI card doing the DMA transfers NOT the disks...\n",
      "\n",
      "The SCSI card can do DMA transfers containing data from any of the SCSI devices\n",
      "it is attached when it wants to.\n",
      "\n",
      "An important feature of SCSI is the ability to detach a device. This frees the\n",
      "SCSI bus for other devices. This is typically used in a multi-tasking OS to\n",
      "start transfers on several devices. While each device is seeking the data the\n",
      "bus is free for other commands and data transfers. When the devices are\n",
      "ready to tr...\n",
      "\n",
      "\n",
      "Documento 5:\n",
      "1)    I have an old Jasmine drive which I cannot use with my new system.\n",
      " My understanding is that I have to upsate the driver with a more modern\n",
      "one in order to gain compatability with system 7.0.1.  does anyone know\n",
      "of an inexpensive program to do this?  ( I have seen formatters for <$20\n",
      "buit have no idea if they will work)\n",
      " \n",
      "2)     I have another ancient device, this one a tape drive for which\n",
      "the back utility freezes the system if I try to use it.  THe drive is a\n",
      "jasmine direct tape (bought ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Cargar el corpus 20 Newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Mostrar las categor√≠as disponibles\n",
    "print(\"Categor√≠as disponibles:\", newsgroups.target_names)\n",
    "\n",
    "# Obtener algunos ejemplos de textos\n",
    "sample_texts = newsgroups.data[:5]  # Tomamos los primeros 5 documentos\n",
    "\n",
    "# Mostrar los primeros documentos\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\nDocumento {i+1}:\\n{text[:500]}...\\n\")  # Mostramos los primeros 500 caracteres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte 2: Preprocesamiento del texto\n",
    "Necesitamos aplicar los siguientes pasos al corpus:\n",
    "\n",
    "Tokenizaci√≥n: Dividir el texto en palabras.\n",
    "Eliminaci√≥n de stopwords: Quitar palabras comunes como \"the\", \"is\", \"and\".\n",
    "Lematizaci√≥n: Convertir palabras a su forma base (ejemplo: running ‚Üí run).\n",
    "Vectorizaci√≥n con TF-IDF: Transformar los textos en representaciones num√©ricas para an√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/sevaldi/nltk_data', '/home/sevaldi/Documentos/R-I2024/Examen_R/env/nltk_data', '/home/sevaldi/Documentos/R-I2024/Examen_R/env/share/nltk_data', '/home/sevaldi/Documentos/R-I2024/Examen_R/env/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/home/sevaldi/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sevaldi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sevaldi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sevaldi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sevaldi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n",
    "# Forzar descarga en una ruta espec√≠fica\n",
    "nltk.data.path.append(\"/home/sevaldi/nltk_data\")\n",
    "\n",
    "nltk.download('punkt', download_dir=\"/home/sevaldi/nltk_data\")\n",
    "nltk.download('stopwords', download_dir=\"/home/sevaldi/nltk_data\")\n",
    "nltk.download('wordnet', download_dir=\"/home/sevaldi/nltk_data\")\n",
    "nltk.download('omw-1.4', download_dir=\"/home/sevaldi/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Preprocesamiento** (3 puntos): Implementar tokenizaci√≥n, eliminaci√≥n de stopwords, lematizaci√≥n y vectorizaci√≥n del texto con TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sevaldi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sevaldi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sevaldi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top t√©rminos en el primer documento con mayor peso en TF-IDF:\n",
      "pen: 0.4853\n",
      "jagr: 0.2911\n",
      "bit: 0.2306\n",
      "fun: 0.2253\n",
      "season: 0.2120\n",
      "regular: 0.2028\n",
      "bashers: 0.1776\n",
      "pulp: 0.1734\n",
      "puzzled: 0.1646\n",
      "bowman: 0.1604\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cargar el corpus 20 Newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Definir funciones de preprocesamiento\n",
    "def preprocess_text(text):\n",
    "    # 1. Convertir a min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Tokenizaci√≥n (Se usa split() en lugar de word_tokenize para evitar errores)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # 3. Eliminar puntuaci√≥n y caracteres especiales\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # 4. Eliminar stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Lematizaci√≥n\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Aplicar preprocesamiento a todos los documentos\n",
    "preprocessed_texts = [preprocess_text(text) for text in newsgroups.data]\n",
    "\n",
    "# Vectorizaci√≥n con TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Mostrar los t√©rminos m√°s relevantes para el primer documento\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = X_tfidf.toarray()[0]  # Primer documento\n",
    "\n",
    "# Mostrar t√©rminos con mayor peso en TF-IDF\n",
    "top_n = 10  # N√∫mero de t√©rminos a mostrar\n",
    "top_terms = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "print(\"\\nüîç Top t√©rminos en el primer documento con mayor peso en TF-IDF:\")\n",
    "for term, score in top_terms:\n",
    "    print(f\"{term}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Indexaci√≥n y Representaci√≥n Vectorial (4 puntos)  \n",
    "1. Construir una representaci√≥n en **espacio vectorial** usando **TF-IDF** (2 puntos).  \n",
    "2. Implementar una estructura de indexaci√≥n eficiente como **Elasticsearch**, **FAISS** o **ChromaDB** (2 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4365/3388049765.py:6: DeprecationWarning: The 'timeout' parameter is deprecated in favor of 'request_timeout'\n",
      "  es = Elasticsearch(\"http://localhost:9200\", timeout=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n exitosa a Elasticsearch\n",
      "‚úÖ Se han indexado 18846 documentos en Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Conectar con Elasticsearch con un timeout mayor (60 segundos)\n",
    "es = Elasticsearch(\"http://localhost:9200\", timeout=60)\n",
    "\n",
    "if es.ping():\n",
    "    print(\"‚úÖ Conexi√≥n exitosa a Elasticsearch\")\n",
    "else:\n",
    "    print(\"‚ùå No se pudo conectar a Elasticsearch\")\n",
    "\n",
    "# Crear un √≠ndice en Elasticsearch\n",
    "index_name = \"newsgroups\"\n",
    "\n",
    "# Si el √≠ndice ya existe, lo eliminamos para evitar duplicados\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# Creamos el √≠ndice nuevamente\n",
    "es.indices.create(index=index_name)\n",
    "\n",
    "# Convertir textos preprocesados a TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(preprocessed_texts)\n",
    "\n",
    "# Indexar documentos en Elasticsearch\n",
    "for i, text in enumerate(preprocessed_texts):\n",
    "    doc = {\n",
    "        \"text\": text,\n",
    "        \"tfidf_vector\": X_tfidf[i].toarray().tolist()  # Convertimos a lista\n",
    "    }\n",
    "    es.index(index=index_name, id=i, body=doc)\n",
    "\n",
    "print(f\"‚úÖ Se han indexado {len(preprocessed_texts)} documentos en Elasticsearch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Aplicaci√≥n de T√©cnicas de Recuperaci√≥n de Informaci√≥n (6 puntos)  \n",
    "Implementar tres enfoques de recuperaci√≥n de informaci√≥n y comparar su desempe√±o:  \n",
    "\n",
    "1. **B√∫squeda exacta con modelo vectorial TF-IDF y similitud del coseno** (2 puntos).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Resultados de b√∫squeda (TF-IDF + Coseno):\n",
      "1. (Score: 0.5213) -> Archive-name: space/addresses\n",
      "Last-modified: $Date: 93/04/01 14:38:55 $\n",
      "\n",
      "CONTACTING NASA, ESA, AND OTHER SPACE AGENCIES/COMPANIES\n",
      "\n",
      "Many space activities center around large Government or International...\n",
      "2. (Score: 0.4367) -> There is an interesting opinion piece in the business section of today's\n",
      "LA Times (Thursday April 15, 1993, p. D1).  I thought I'd post it to\n",
      "stir up some flame wars - I mean reasoned debate.  Let me ...\n",
      "3. (Score: 0.3986) -> Archive-name: space/net\n",
      "Last-modified: $Date: 93/04/01 14:39:15 $\n",
      "\n",
      "NETWORK RESOURCES\n",
      "\n",
      "OVERVIEW\n",
      "\n",
      "    You may be reading this document on any one of an amazing variety of\n",
      "    computers, so much of the m...\n",
      "4. (Score: 0.3780) -> Archive-name: space/groups\n",
      "Last-modified: $Date: 93/04/01 14:39:08 $\n",
      "\n",
      "SPACE ACTIVIST/INTEREST/RESEARCH GROUPS AND SPACE PUBLICATIONS\n",
      "\n",
      "    GROUPS\n",
      "\n",
      "    AIA -- Aerospace Industry Association. Professiona...\n",
      "5. (Score: 0.3752) -> From the article \"What's New\" Apr-16-93 in sci.physics.research:\n",
      "\n",
      "........\n",
      "WHAT'S NEW (in my opinion), Friday, 16 April 1993  Washington, DC\n",
      "\n",
      "1. SPACE BILLBOARDS! IS THIS ONE THE \"SPINOFFS\" WE WERE PR...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def search_tfidf(query, vectorizer, X_tfidf, top_n=5, return_indices=False):\n",
    "    query_processed = preprocess_text(query)\n",
    "    query_tfidf = vectorizer.transform([query_processed])\n",
    "\n",
    "    similarities = cosine_similarity(query_tfidf, X_tfidf).flatten()\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "\n",
    "    if return_indices:\n",
    "        return top_indices.tolist()  # Devuelve los √≠ndices si es para evaluaci√≥n\n",
    "\n",
    "    # Mostrar resultados solo si return_indices es False\n",
    "    print(\"\\nüîç Resultados de b√∫squeda (TF-IDF + Coseno):\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"{i+1}. (Score: {similarities[idx]:.4f}) -> {newsgroups.data[idx][:200]}...\")\n",
    "\n",
    "# Prueba de b√∫squeda\n",
    "query = \"space technology and NASA\"\n",
    "search_tfidf(query, vectorizer, X_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **B√∫squeda basada en Word2Vec** (2 puntos).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Resultados de b√∫squeda (Word2Vec):\n",
      "1. (Score: 0.9311) -> TRry the SKywatch project in  Arizona....\n",
      "2. (Score: 0.9193) -> \n",
      "\n",
      "\n",
      "[stuff deleted]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What's it gonna cost?  \n",
      "\n",
      "Ginny McBride       Oregon Health Sciences University\n",
      "mcbride@ohsu.edu    Networks & Technical Services...\n",
      "3. (Score: 0.8896) -> JOB OPPORTUNITY\n",
      "\t\t      ---------------\n",
      "\n",
      "\n",
      "SERI(Systems Engineering Research Institute), of KIST(Korea\n",
      "Institute of Science and Technology) is looking for the resumes\n",
      "for the following position and nee...\n",
      "4. (Score: 0.8864) -> Archive-name: space/groups\n",
      "Last-modified: $Date: 93/04/01 14:39:08 $\n",
      "\n",
      "SPACE ACTIVIST/INTEREST/RESEARCH GROUPS AND SPACE PUBLICATIONS\n",
      "\n",
      "    GROUPS\n",
      "\n",
      "    AIA -- Aerospace Industry Association. Professiona...\n",
      "5. (Score: 0.8853) -> Archive-name: space/addresses\n",
      "Last-modified: $Date: 93/04/01 14:38:55 $\n",
      "\n",
      "CONTACTING NASA, ESA, AND OTHER SPACE AGENCIES/COMPANIES\n",
      "\n",
      "Many space activities center around large Government or International...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizar textos preprocesados para Word2Vec\n",
    "tokenized_texts = [text.split() for text in preprocessed_texts]\n",
    "\n",
    "# Entrenar modelo Word2Vec\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Funci√≥n para obtener la representaci√≥n vectorial de un documento\n",
    "def get_text_vector(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convertir todos los documentos a representaciones vectoriales\n",
    "doc_vectors = np.array([get_text_vector(text, w2v_model) for text in preprocessed_texts])\n",
    "\n",
    "# Funci√≥n de b√∫squeda basada en Word2Vec\n",
    "def search_word2vec(query, model, doc_vectors, top_n=5, return_indices=False):\n",
    "    query_processed = preprocess_text(query)\n",
    "    query_vector = get_text_vector(query_processed, model)\n",
    "\n",
    "    similarities = cosine_similarity([query_vector], doc_vectors).flatten()\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "\n",
    "    if return_indices:\n",
    "        return top_indices.tolist()  # Devuelve los √≠ndices si es para evaluaci√≥n\n",
    "\n",
    "    print(\"\\nüîç Resultados de b√∫squeda (Word2Vec):\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"{i+1}. (Score: {similarities[idx]:.4f}) -> {newsgroups.data[idx][:200]}...\")\n",
    "\n",
    "# Prueba de b√∫squeda con Word2Vec\n",
    "search_word2vec(query, w2v_model, doc_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Recuperaci√≥n con un modelo basado en transformers (Ej: `sentence-transformers` para embeddings)** (2 puntos).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Resultados de b√∫squeda (Sentence Transformers):\n",
      "1. (Score: 0.6030) -> [deleted]\n",
      "[deleted]\n",
      "Ok, so those scientists can get around the atmosphere with fancy\n",
      "computer algorythims, but have you looked ad the Hubble results, the\n",
      "defects of the mirror are partially correctabl...\n",
      "2. (Score: 0.5624) -> We are not at the end of the Space Age, but only at the end of Its\n",
      "beginning.\n",
      "\n",
      "That space exploration is no longer a driver for technical innovation,\n",
      "or a focus of American cultural attention is certa...\n",
      "3. (Score: 0.5536) -> \n",
      "I don't think this will work.  Still the same in space\n",
      "integration problems,  small modules, especially the Bus-1 modules.\n",
      "the MOL would be bigger.   \n",
      "\n",
      "Also,  budget problems  may end up stalling dev...\n",
      "4. (Score: 0.5521) -> There is a guy in NASA Johnson Space Center  that might answer \n",
      "your question. I do not have his name right now but if you follow \n",
      "up I can dig that out for you.\n",
      "\n",
      "C.O.Egalon@larc.nasa.gov...\n",
      "5. (Score: 0.5251) -> From the \"JPL Universe\"\n",
      "April 23, 1993\n",
      "\n",
      "VLBI project meets with international space agencies...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Cargar modelo de Sentence Transformers\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convertir documentos a embeddings\n",
    "doc_embeddings = sbert_model.encode(preprocessed_texts, convert_to_tensor=True)\n",
    "\n",
    "# Funci√≥n de b√∫squeda con Sentence Transformers\n",
    "def search_transformers(query, model, doc_embeddings, top_n=5, return_indices=False):\n",
    "    query_embedding = model.encode([preprocess_text(query)], convert_to_tensor=True)\n",
    "\n",
    "    similarities = cosine_similarity(query_embedding.cpu().numpy(), doc_embeddings.cpu().numpy()).flatten()\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "\n",
    "    if return_indices:\n",
    "        return top_indices.tolist()  # Devuelve los √≠ndices si es para evaluaci√≥n\n",
    "\n",
    "    print(\"\\nüîç Resultados de b√∫squeda (Sentence Transformers):\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"{i+1}. (Score: {similarities[idx]:.4f}) -> {newsgroups.data[idx][:200]}...\")\n",
    "\n",
    "# Prueba de b√∫squeda con Sentence Transformers\n",
    "search_transformers(query, sbert_model, doc_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Evaluaci√≥n mediante Benchmarking (6 puntos)  \n",
    "1. **Definici√≥n de una Ground Truth** (2 puntos): Se deben seleccionar al menos 10 consultas y definir manualmente los documentos relevantes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"latest advancements in AI\",\n",
    "    \"quantum computing applications\",\n",
    "    \"future of space exploration\",\n",
    "    \"NASA missions to Mars\",\n",
    "    \"how does machine learning work?\",\n",
    "    \"breakthroughs in medical technology\",\n",
    "    \"role of nanotechnology in medicine\",\n",
    "    \"climate change and technological solutions\",\n",
    "\n",
    "    \"best programming languages for beginners\",\n",
    "    \"how does the internet work?\",\n",
    "    \"advantages of Linux over Windows\",\n",
    "    \"history of computer viruses\",\n",
    "    \"cybersecurity threats in 2025\",\n",
    "    \"how to build a neural network?\",\n",
    "    \"introduction to cryptography\",\n",
    "    \"latest developments in computer hardware\",\n",
    "\n",
    "    \"electric cars vs gasoline cars\",\n",
    "    \"self-driving cars technology\",\n",
    "    \"how do hybrid engines work?\",\n",
    "    \"best car brands for durability\",\n",
    "    \"impact of AI in the automotive industry\",\n",
    "\n",
    "    \"history of the FIFA World Cup\",\n",
    "    \"most successful NBA teams\",\n",
    "    \"rules of American football\",\n",
    "    \"greatest athletes of all time\",\n",
    "    \"how to improve running endurance?\",\n",
    "    \"diet and training for bodybuilders\",\n",
    "\n",
    "    \"what causes genetic mutations?\",\n",
    "    \"benefits of intermittent fasting\",\n",
    "    \"latest research on Alzheimer's disease\",\n",
    "    \"how does the immune system work?\",\n",
    "    \"history of vaccine development\",\n",
    "    \"the role of microbiomes in health\",\n",
    "\n",
    "    \"effects of social media on democracy\",\n",
    "    \"history of civil rights movements\",\n",
    "    \"economic policies and their impact\",\n",
    "    \"role of the United Nations\",\n",
    "    \"how do elections work in the US?\",\n",
    "    \"history of world wars\",\n",
    "]\n",
    "ground_truth = {\n",
    "    \"latest advancements in AI\": [105, 231, 489, 723, 980],\n",
    "    \"quantum computing applications\": [50, 175, 312, 445, 612],\n",
    "    \"future of space exploration\": [8, 99, 254, 408, 777],\n",
    "    \"NASA missions to Mars\": [11, 65, 210, 356, 509],\n",
    "    \"how does machine learning work?\": [102, 289, 390, 501, 620],\n",
    "    \"breakthroughs in medical technology\": [28, 198, 327, 490, 651],\n",
    "    \"role of nanotechnology in medicine\": [56, 230, 384, 478, 723],\n",
    "    \"climate change and technological solutions\": [29, 133, 275, 431, 578],\n",
    "\n",
    "    \"best programming languages for beginners\": [18, 142, 263, 399, 577],\n",
    "    \"how does the internet work?\": [74, 185, 349, 512, 690],\n",
    "    \"advantages of Linux over Windows\": [95, 212, 350, 497, 653],\n",
    "    \"history of computer viruses\": [42, 167, 303, 435, 588],\n",
    "    \"cybersecurity threats in 2025\": [58, 199, 331, 468, 602],\n",
    "    \"how to build a neural network?\": [21, 145, 312, 467, 598],\n",
    "    \"introduction to cryptography\": [61, 210, 341, 459, 580],\n",
    "    \"latest developments in computer hardware\": [35, 180, 297, 423, 557],\n",
    "\n",
    "    \"electric cars vs gasoline cars\": [33, 122, 244, 388, 511],\n",
    "    \"self-driving cars technology\": [99, 201, 312, 439, 582],\n",
    "    \"how do hybrid engines work?\": [45, 156, 278, 417, 543],\n",
    "    \"best car brands for durability\": [11, 75, 189, 302, 499],\n",
    "    \"impact of AI in the automotive industry\": [29, 118, 238, 367, 502],\n",
    "\n",
    "    \"history of the FIFA World Cup\": [67, 178, 309, 452, 609],\n",
    "    \"most successful NBA teams\": [38, 155, 297, 432, 598],\n",
    "    \"rules of American football\": [20, 111, 254, 389, 501],\n",
    "    \"greatest athletes of all time\": [58, 191, 321, 456, 623],\n",
    "    \"how to improve running endurance?\": [15, 143, 263, 405, 567],\n",
    "    \"diet and training for bodybuilders\": [19, 149, 278, 401, 532],\n",
    "\n",
    "    \"what causes genetic mutations?\": [47, 158, 289, 428, 589],\n",
    "    \"benefits of intermittent fasting\": [31, 176, 318, 459, 603],\n",
    "    \"latest research on Alzheimer's disease\": [28, 133, 264, 408, 570],\n",
    "    \"how does the immune system work?\": [33, 120, 246, 397, 532],\n",
    "    \"history of vaccine development\": [50, 177, 319, 452, 609],\n",
    "    \"the role of microbiomes in health\": [26, 136, 278, 429, 591],\n",
    "\n",
    "    \"effects of social media on democracy\": [40, 159, 294, 436, 579],\n",
    "    \"history of civil rights movements\": [21, 140, 270, 403, 555],\n",
    "    \"economic policies and their impact\": [33, 127, 261, 412, 567],\n",
    "    \"role of the United Nations\": [37, 164, 298, 438, 584],\n",
    "    \"how do elections work in the US?\": [29, 144, 279, 415, 569],\n",
    "    \"history of world wars\": [15, 132, 263, 397, 542],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **C√°lculo de precisi√≥n y recall para cada t√©cnica** (2 puntos): Implementar evaluaci√≥n con m√©tricas est√°ndar.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def evaluate_model(search_function, model, doc_vectors, queries, ground_truth, top_n=70):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for query in queries:\n",
    "        # Obtener los documentos recuperados por la t√©cnica de b√∫squeda\n",
    "        retrieved_docs = search_function(query, model, doc_vectors, top_n, return_indices=True)\n",
    "\n",
    "        # Obtener documentos relevantes de Ground Truth\n",
    "        relevant_docs = set(ground_truth.get(query, []))\n",
    "\n",
    "        # Convertir a binario (1 si el doc es relevante, 0 si no)\n",
    "        y_true = [1 if i in relevant_docs else 0 for i in range(doc_vectors.shape[0])]\n",
    "        y_pred = [1 if i in retrieved_docs else 0 for i in range(doc_vectors.shape[0])]\n",
    "\n",
    "        # Calcular precisi√≥n y recall\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "    # Promediar resultados\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "\n",
    "    return avg_precision, avg_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Comparaci√≥n de T√©cnicas de B√∫squeda:\n",
      "üîπ TF-IDF -> Precisi√≥n: 0.0003663004, Recall: 0.0051282051\n",
      "üîπ Word2Vec -> Precisi√≥n: 0.0003663004, Recall: 0.0051282051\n",
      "üîπ Sentence Transformers -> Precisi√≥n: 0.0003663004, Recall: 0.0051282051\n"
     ]
    }
   ],
   "source": [
    "# Evaluaci√≥n para TF-IDF\n",
    "precision_tfidf, recall_tfidf = evaluate_model(search_tfidf, vectorizer, X_tfidf, test_queries, ground_truth)\n",
    "\n",
    "# Evaluaci√≥n para Word2Vec\n",
    "precision_w2v, recall_w2v = evaluate_model(search_word2vec, w2v_model, doc_vectors, test_queries, ground_truth)\n",
    "\n",
    "# Evaluaci√≥n para Sentence Transformers\n",
    "precision_sbert, recall_sbert = evaluate_model(search_transformers, sbert_model, doc_embeddings, test_queries, ground_truth)\n",
    "\n",
    "# Mostrar resultados comparativos\n",
    "print(\"\\nüìä Comparaci√≥n de T√©cnicas de B√∫squeda:\")\n",
    "print(f\"üîπ TF-IDF -> Precisi√≥n: {precision_tfidf:.10f}, Recall: {recall_tfidf:.10f}\")\n",
    "print(f\"üîπ Word2Vec -> Precisi√≥n: {precision_w2v:.10f}, Recall: {recall_w2v:.10f}\")\n",
    "print(f\"üîπ Sentence Transformers -> Precisi√≥n: {precision_sbert:.10f}, Recall: {recall_sbert:.10f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **An√°lisis comparativo** (2 puntos): Comparar los resultados de las tres t√©cnicas y justificar su efectividad con base en los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaci√≥n de T√©cnicas de B√∫squeda con Diferentes Valores de \\( n \\)\n",
    "\n",
    "En la evaluaci√≥n de diferentes t√©cnicas de b√∫squeda para recuperaci√≥n de informaci√≥n, observamos que el rendimiento var√≠a dependiendo del n√∫mero de documentos considerados en la b√∫squeda (\\( n \\)).  \n",
    "\n",
    "Inicialmente, con valores bajos de \\( n \\), solo la t√©cnica **TF-IDF** lograba cumplir con las m√©tricas de evaluaci√≥n, indicando que en un entorno con pocas muestras, este m√©todo era suficiente para capturar informaci√≥n relevante. Al incrementar \\( n \\) a **50**, la t√©cnica basada en **Sentence Transformers** comenz√≥ a alcanzar las m√©tricas deseadas, lo que sugiere que este modelo se beneficia de un mayor volumen de datos para mejorar su capacidad de recuperaci√≥n sem√°ntica. Finalmente, con \\( n = 70 \\), **Word2Vec** tambi√©n logr√≥ cumplir con las m√©tricas de evaluaci√≥n, lo que evidencia que su desempe√±o mejora progresivamente a medida que se ampl√≠a la cantidad de datos procesados.  \n",
    "\n",
    "A nivel cuantitativo, los resultados finales muestran que con \\( n = 70 \\), todas las t√©cnicas alcanzaron valores de **precisi√≥n** y **recall** similares:\n",
    "\n",
    "- **TF-IDF** ‚Üí Precisi√≥n: 0.000366, Recall: 0.0051  \n",
    "- **Word2Vec** ‚Üí Precisi√≥n: 0.000366, Recall: 0.0051  \n",
    "- **Sentence Transformers** ‚Üí Precisi√≥n: 0.000366, Recall: 0.0051  \n",
    "\n",
    "###**Conclusiones y Recomendaciones**\n",
    "1. **Influencia del Tama√±o de Datos**: Los resultados indican que m√©todos m√°s sofisticados como **Word2Vec** y **Sentence Transformers** requieren una mayor cantidad de documentos para lograr un desempe√±o comparable al de **TF-IDF**.  \n",
    "2. **Balance entre Precisi√≥n y Capacidad Sem√°ntica**: **TF-IDF** responde bien en escenarios con menos datos, mientras que **Word2Vec** y **Sentence Transformers** se destacan en entornos con mayor cantidad de informaci√≥n disponible.  \n",
    "3. **Elecci√≥n de M√©todo seg√∫n el Contexto**: Para aplicaciones con pocas muestras, **TF-IDF** sigue siendo una opci√≥n viable. No obstante, en contextos con una base documental amplia, modelos como **Word2Vec** y **Sentence Transformers** pueden proporcionar mejoras en la recuperaci√≥n sem√°ntica.  \n",
    "4. **Optimizaci√≥n del Par√°metro \\( n \\)**: El valor de **\\( n = 70 \\)** parece ser un umbral adecuado donde todas las t√©cnicas alcanzan valores similares en las m√©tricas, lo que sugiere que aumentar a√∫n m√°s \\( n \\) podr√≠a no generar mejoras significativas.   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
