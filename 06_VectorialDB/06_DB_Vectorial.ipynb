{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 06: Base de Datos Vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1: Recuperación con TF-IDF\n",
    "\n",
    "**1. Carga los datos en Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carga el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(\"../data/wiki_movie_plots_deduped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title  \\\n",
      "0            Kansas Saloon Smashers   \n",
      "1     Love by the Light of the Moon   \n",
      "2           The Martyred Presidents   \n",
      "3  Terrible Teddy, the Grizzly King   \n",
      "4            Jack and the Beanstalk   \n",
      "\n",
      "                                                Plot  \n",
      "0  A bartender is working at a saloon, serving dr...  \n",
      "1  The moon, painted with a smiling face hangs ov...  \n",
      "2  The film, just over a minute long, is composed...  \n",
      "3  Lasting just 61 seconds and consisting of two ...  \n",
      "4  The earliest known adaptation of the classic f...  \n"
     ]
    }
   ],
   "source": [
    "# Filtra únicamente las columnas Title y Plot\n",
    "df_filtered = df[['Title', 'Plot']]\n",
    "\n",
    "# Muestra las primeras filas del DataFrame filtrado\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Title  \\\n",
      "0            Kansas Saloon Smashers   \n",
      "1     Love by the Light of the Moon   \n",
      "2           The Martyred Presidents   \n",
      "3  Terrible Teddy, the Grizzly King   \n",
      "4            Jack and the Beanstalk   \n",
      "\n",
      "                                                Plot  \n",
      "0  A bartender is working at a saloon, serving dr...  \n",
      "1  The moon, painted with a smiling face hangs ov...  \n",
      "2  The film, just over a minute long, is composed...  \n",
      "3  Lasting just 61 seconds and consisting of two ...  \n",
      "4  The earliest known adaptation of the classic f...  \n"
     ]
    }
   ],
   "source": [
    "# Filtra únicamente las columnas Title y Plot\n",
    "df_filtered = df[['Title', 'Plot']]\n",
    "\n",
    "# Muestra las primeras filas del DataFrame filtrado\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Configurar TF-IDF**\n",
    "\n",
    "- usa la libreria scikit-lear para calcular los puntajes TF-IDF de los plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m\n\u001b[0;32m     18\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m     19\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,  \u001b[38;5;66;03m# Limita el vocabulario\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     preprocessor\u001b[38;5;241m=\u001b[39mclean_text,  \u001b[38;5;66;03m# Aplica la limpieza personalizada\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[a-z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Solo considera palabras de al menos 2 letras\u001b[39;00m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Calcula los puntajes TF-IDF para los Plots\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Convierte la matriz dispersa en un DataFrame\u001b[39;00m\n\u001b[0;32m     28\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     29\u001b[0m     tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(),\n\u001b[0;32m     30\u001b[0m     columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out(),\n\u001b[0;32m     31\u001b[0m     index\u001b[38;5;241m=\u001b[39mdf_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2138\u001b[0m )\n\u001b[1;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1277\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1275\u001b[0m feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m         feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n\u001b[0;32m   1279\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Función para limpiar texto\n",
    "def clean_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar tildes\n",
    "    text = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "    # Eliminar números, puntuaciones y caracteres no alfabéticos\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Inicializa el vectorizador TF-IDF con el preprocesador personalizado\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limita el vocabulario\n",
    "    preprocessor=clean_text,  # Aplica la limpieza personalizada\n",
    "    token_pattern=r'\\b[a-z]{2,}\\b'  # Solo considera palabras de al menos 2 letras\n",
    ")\n",
    "\n",
    "# Calcula los puntajes TF-IDF para los Plots\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_filtered['Plot'].fillna(''))\n",
    "\n",
    "# Convierte la matriz dispersa en un DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=df_filtered['Title']\n",
    ")\n",
    "\n",
    "# Muestra las primeras filas del DataFrame TF-IDF\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Realizar Consultas:**\n",
    "\n",
    "- Escribe una función que calculo la similitud entre una consulta y los documentos usando la matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Title  Similarity\n",
      "18255                 The Medicine Man    0.452338\n",
      "18965                Death Is a Number    0.440332\n",
      "20811       My Wrongs #8245–8249 & 117    0.392925\n",
      "22406                        King Dave    0.335430\n",
      "18390            The Man in the Mirror    0.331631\n",
      "1942                  Murder in Harlem    0.330803\n",
      "15734                         The Road    0.324203\n",
      "16472  Cheech & Chong's Animated Movie    0.316461\n",
      "23866              Everyday I Love You    0.314217\n",
      "27894                    Junior Senior    0.307387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calcular_similitud(query, tfidf_vectorizer, tfidf_matrix, titles):\n",
    "    # Limpia la consulta usando la misma lógica de preprocesamiento\n",
    "    query_cleaned = clean_text(query)\n",
    "\n",
    "    # Vectoriza la consulta\n",
    "    query_tfidf = tfidf_vectorizer.transform([query_cleaned])\n",
    "\n",
    "    # Calcula la similitud coseno entre la consulta y los documentos\n",
    "    similitudes = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "    # Crea un DataFrame con los resultados\n",
    "    resultados = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Similarity': similitudes\n",
    "    })\n",
    "\n",
    "    # Ordena los documentos por similitud en orden descendente\n",
    "    resultados_ordenados = resultados.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "    return resultados_ordenados\n",
    "\n",
    "# Ejemplo de uso\n",
    "consulta = \"man\"\n",
    "resultados = calcular_similitud(consulta, tfidf_vectorizer, tfidf_matrix, df_filtered['Title'])\n",
    "\n",
    "# Muestra los resultados\n",
    "print(resultados.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluar los resultados:**\n",
    "\n",
    "- Registra los documentos recuperados y analiza su relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: data\\resultados_recuperados.csv\n",
      "Documentos relevantes (similitud >= 0.4): 2\n",
      "Documentos relevantes:\n",
      "                   Title  Similarity\n",
      "18255   The Medicine Man    0.452338\n",
      "18965  Death Is a Number    0.440332\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def registrar_documentos(resultados, output_path=\"resultados_recuperados.csv\", threshold=0.5):\n",
    "    # Asegúrate de que el directorio existe\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Marca los documentos como relevantes o no según el umbral\n",
    "    resultados['Relevancia'] = resultados['Similarity'] >= threshold\n",
    "\n",
    "    # Guarda los resultados en un archivo CSV\n",
    "    resultados.to_csv(output_path, index=False)\n",
    "    print(f\"Resultados guardados en: {output_path}\")\n",
    "\n",
    "    # Filtra los documentos relevantes\n",
    "    resultados_filtrados = resultados[resultados['Relevancia']]\n",
    "    print(f\"Documentos relevantes (similitud >= {threshold}): {len(resultados_filtrados)}\")\n",
    "\n",
    "    return resultados_filtrados\n",
    "\n",
    "# Llama a la función con los resultados de la consulta\n",
    "ruta_salida = os.path.join(\"data\", \"resultados_recuperados.csv\")\n",
    "documentos_relevantes = registrar_documentos(resultados, output_path=ruta_salida, threshold=0.4)\n",
    "\n",
    "# Analiza los resultados relevantes\n",
    "print(\"Documentos relevantes:\")\n",
    "print(documentos_relevantes[['Title', 'Similarity']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2: Recuperación con BM25\n",
    "\n",
    "**1. Configurar Elasticsearch:**\n",
    "\n",
    "- Reutiliza el índice creado en el Ejercicio 1 para realizar consultas basadas en BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Instalar Whoosh o Rank-BM25:**\n",
    "\n",
    "Para usar BM25 en Python, podemos usar rank_bm25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\diego\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from rank-bm25) (1.23.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Implementar Recuperación con BM25:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la consulta con BM25:\n",
      "Resultado 1:\n",
      "Título: For Her Sake\n",
      "Plot: The film is a period drama taking place right before the start of the American Civil War. A young Southern girl chooses between two suitors. She chooses the man who goes to fight Stars and Bars of the Confederacy whilst the rejected suitor goes to fight for the Union. During the war, the Confederate soldier is captured and brought before the Union officer who recognizes him as his rival. The Union man is cruel to his rival and tries to break his spirit with harsh treatment. The girl hears of his plight and becomes determined to rescue him. She evades the guards and gives her lover a file to free himself from the bars. Together they flee and are discovered in the final moments of their escape. One of the sentries shoots at the man, but his shot misses and the two flee on horseback.[1] The Union officer is enraged by the escape and tracks the pair to the girl's home just over the Federal line. He sets up guards around the house and enters alone to take them prisoner by his own hand. He makes his way through the house and breaks down the doors to find the man he wants. Upon finding the man, he does not arrest him - for the Confederate soldier is grief-stricken and bending over the body of his fiancée. The bullet the sentry shot at him instead took her life. Together the two rivals mourn her death, and the Union officer leaves without arresting his rival - for her sake.[1]\n",
      "Score: 1.7688889868792632\n",
      "Resultado 2:\n",
      "Título: Kansas Saloon Smashers\n",
      "Plot: A bartender is working at a saloon, serving drinks to customers. After he fills a stereotypically Irish man's bucket with beer, Carrie Nation and her followers burst inside. They assault the Irish man, pulling his hat over his eyes and then dumping the beer over his head. The group then begin wrecking the bar, smashing the fixtures, mirrors, and breaking the cash register. The bartender then sprays seltzer water in Nation's face before a group of policemen appear and order everybody to leave.[1]\n",
      "Score: 1.6913198285442161\n",
      "Resultado 3:\n",
      "Título: The Musketeers of Pig Alley\n",
      "Plot: The film is about a poor married couple living in New York City. The husband works as a musician and must often travel for work. When returning, his wallet is taken by a gangster. His wife goes to a ball where a man tries to drug her, but his attempt is stopped by the same man who robbed the husband. The two criminals become rivals, and a shootout ensues. The husband gets caught in the shootout and recognizes one of the men as the gangster who took his money. The husband sneaks his wallet back and the gangster goes to safety in the couple's apartment. Policemen track the gangster down but the wife gives him a false alibi.\n",
      "Score: 1.5852654769094259\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Asegurar que nltk tenga los recursos necesarios\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizar los documentos (convertir texto en listas de palabras)\n",
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Crear el modelo BM25\n",
    "bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "# Definir la consulta y tokenizarla\n",
    "query = \"man\"\n",
    "tokenized_query = word_tokenize(query.lower())\n",
    "\n",
    "# Obtener los 3 documentos más relevantes\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "top_n = 3\n",
    "top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Resultados de la consulta con BM25:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"Resultado {i+1}:\")\n",
    "    print(f\"Título: {metadatas[idx]['Title']}\")\n",
    "    print(f\"Plot: {documents[idx]}\")\n",
    "    print(f\"Score: {scores[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3: Recuperación con FAISS\n",
    "\n",
    "**1. Configurar FAISS:**\n",
    "\n",
    "Crear un índice en FAISS y agregar los embeddings generados previamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Instalar FAISS:**\n",
    "\n",
    "Si aún no lo tienes instalado, ejecútalo con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp38-cp38-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\diego\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from faiss-cpu) (1.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\diego\\appdata\\roaming\\python\\python38\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp38-cp38-win_amd64.whl (14.6 MB)\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.6 MB 640.0 kB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.0/14.6 MB 388.9 kB/s eta 0:00:38\n",
      "   ---------------------------------------- 0.1/14.6 MB 930.9 kB/s eta 0:00:16\n",
      "    --------------------------------------- 0.2/14.6 MB 1.1 MB/s eta 0:00:13\n",
      "    --------------------------------------- 0.3/14.6 MB 1.5 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.6/14.6 MB 2.0 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.7/14.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/14.6 MB 2.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.3/14.6 MB 3.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.2/14.6 MB 4.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.1/14.6 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.6 MB 7.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.2/14.6 MB 7.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.2/14.6 MB 7.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.3/14.6 MB 8.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.3/14.6 MB 8.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.3/14.6 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.3/14.6 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.4/14.6 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.4/14.6 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.4/14.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.4/14.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.5/14.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.5/14.6 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.5/14.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.6/14.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.6/14.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.6/14.6 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.6/14.6 MB 13.3 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Configurar FAISS y agregar los embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han agregado 50 vectores al índice FAISS.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convertir los embeddings a un array numpy (float32 requerido por FAISS)\n",
    "embedding_dim = len(embeddings[0])  # Dimensión de los embeddings\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # Crear un índice de FAISS basado en L2 (distancia euclidiana)\n",
    "\n",
    "# Agregar los embeddings al índice\n",
    "index.add(np.array(embeddings, dtype=np.float32))\n",
    "\n",
    "print(f\"Se han agregado {index.ntotal} vectores al índice FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Realizar una búsqueda en FAIS:**\n",
    "\n",
    "Para buscar los documentos más similares a una consulta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la consulta con FAISS:\n",
      "Resultado 1:\n",
      "Título: The Black Viper\n",
      "Plot: A thug accosts a girl as she leaves her workplace but a man rescues her. The thug vows revenge and, with the help of two friends, attacks the girl and her rescuer again as they're going for a walk. This time they succeed in kidnapping the rescuer. He is bound and gagged and taken away in a cart. The girl runs home and gets help from several neighbors. They track the ruffians down to a cabin in the mountains where the gang has trapped their victim and set the cabin on fire. A thug and Rescuer fight on the roof of the house.\n",
      "Distancia: 1.6032384634017944\n",
      "Resultado 2:\n",
      "Título: Petticoat Camp\n",
      "Plot: Only lasting 15 minutes, it is a light-hearted comedy about the battle between the sexes as several married couples go on a camp-out together. The women soon realize that the men expect them to do perform all of the work while they relax, leading to several comedic situations.\n",
      "Distancia: 1.6491906642913818\n",
      "Resultado 3:\n",
      "Título: How Brown Saw the Baseball Game\n",
      "Plot: Before heading out to a baseball game at a nearby ballpark, sports fan Mr. Brown drinks several highball cocktails. He arrives at the ballpark to watch the game, but has become so inebriated that the game appears to him in reverse, with the players running the bases backwards and the baseball flying back into the pitcher's hand. After the game is over, Mr. Brown is escorted home by one of his friends. When they arrive at Brown's house, they encounter his wife who becomes furious with the friend and proceeds to physically assault him, believing he is responsible for her husband's severe intoxication.[1]\n",
      "Distancia: 1.6648643016815186\n"
     ]
    }
   ],
   "source": [
    "# Definir la consulta\n",
    "query = \"man\"\n",
    "\n",
    "# Generar el embedding de la consulta\n",
    "query_embedding = model.encode([query]).astype(np.float32)\n",
    "\n",
    "# Buscar los 3 documentos más similares\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Resultados de la consulta con FAISS:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Resultado {i+1}:\")\n",
    "    print(f\"Título: {metadatas[idx]['Title']}\")\n",
    "    print(f\"Plot: {documents[idx]}\")\n",
    "    print(f\"Distancia: {distances[0][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 4: Recuperación con ChromaDB\n",
    "\n",
    "**1. Configurar ChromaDB:**\n",
    "\n",
    "Inicia una base de datos de ChromaDB y define el esquema con los campos Tittle, Plot y Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Preparar los datos**\n",
    "Seleccionamos la columna que contiene el texto (por ejemplo, Plot) y una columna como identificadores únicos (ID o similar). Si no hay una columna de IDs, podemos generarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34881</th>\n",
       "      <td>2014</td>\n",
       "      <td>The Water Diviner</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Director: Russell Crowe</td>\n",
       "      <td>Director: Russell Crowe\\r\\nCast: Russell Crowe...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Water_Diviner</td>\n",
       "      <td>The film begins in 1919, just after World War ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34882</th>\n",
       "      <td>2017</td>\n",
       "      <td>Çalgı Çengi İkimiz</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Selçuk Aydemir</td>\n",
       "      <td>Ahmet Kural, Murat Cemcir</td>\n",
       "      <td>comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/%C3%87alg%C4%B1_...</td>\n",
       "      <td>Two musicians, Salih and Gürkan, described the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34883</th>\n",
       "      <td>2017</td>\n",
       "      <td>Olanlar Oldu</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Hakan Algül</td>\n",
       "      <td>Ata Demirer, Tuvana Türkay, Ülkü Duru</td>\n",
       "      <td>comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Olanlar_Oldu</td>\n",
       "      <td>Zafer, a sailor living with his mother Döndü i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34884</th>\n",
       "      <td>2017</td>\n",
       "      <td>Non-Transferable</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Brendan Bradley</td>\n",
       "      <td>YouTubers Shanna Malcolm, Shira Lazar, Sara Fl...</td>\n",
       "      <td>romantic comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Non-Transferable...</td>\n",
       "      <td>The film centres around a young woman named Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34885</th>\n",
       "      <td>2017</td>\n",
       "      <td>İstanbul Kırmızısı</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Ferzan Özpetek</td>\n",
       "      <td>Halit Ergenç, Tuba Büyüküstün, Mehmet Günsür, ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>https://en.wikipedia.org/wiki/%C4%B0stanbul_K%...</td>\n",
       "      <td>The writer Orhan Şahin returns to İstanbul aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34886 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Release Year                             Title Origin/Ethnicity  \\\n",
       "0              1901            Kansas Saloon Smashers         American   \n",
       "1              1901     Love by the Light of the Moon         American   \n",
       "2              1901           The Martyred Presidents         American   \n",
       "3              1901  Terrible Teddy, the Grizzly King         American   \n",
       "4              1902            Jack and the Beanstalk         American   \n",
       "...             ...                               ...              ...   \n",
       "34881          2014                 The Water Diviner          Turkish   \n",
       "34882          2017                Çalgı Çengi İkimiz          Turkish   \n",
       "34883          2017                      Olanlar Oldu          Turkish   \n",
       "34884          2017                  Non-Transferable          Turkish   \n",
       "34885          2017                İstanbul Kırmızısı          Turkish   \n",
       "\n",
       "                                 Director  \\\n",
       "0                                 Unknown   \n",
       "1                                 Unknown   \n",
       "2                                 Unknown   \n",
       "3                                 Unknown   \n",
       "4      George S. Fleming, Edwin S. Porter   \n",
       "...                                   ...   \n",
       "34881             Director: Russell Crowe   \n",
       "34882                      Selçuk Aydemir   \n",
       "34883                         Hakan Algül   \n",
       "34884                     Brendan Bradley   \n",
       "34885                      Ferzan Özpetek   \n",
       "\n",
       "                                                    Cast            Genre  \\\n",
       "0                                                    NaN          unknown   \n",
       "1                                                    NaN          unknown   \n",
       "2                                                    NaN          unknown   \n",
       "3                                                    NaN          unknown   \n",
       "4                                                    NaN          unknown   \n",
       "...                                                  ...              ...   \n",
       "34881  Director: Russell Crowe\\r\\nCast: Russell Crowe...          unknown   \n",
       "34882                          Ahmet Kural, Murat Cemcir           comedy   \n",
       "34883              Ata Demirer, Tuvana Türkay, Ülkü Duru           comedy   \n",
       "34884  YouTubers Shanna Malcolm, Shira Lazar, Sara Fl...  romantic comedy   \n",
       "34885  Halit Ergenç, Tuba Büyüküstün, Mehmet Günsür, ...         romantic   \n",
       "\n",
       "                                               Wiki Page  \\\n",
       "0      https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1      https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2      https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3      https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4      https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "...                                                  ...   \n",
       "34881    https://en.wikipedia.org/wiki/The_Water_Diviner   \n",
       "34882  https://en.wikipedia.org/wiki/%C3%87alg%C4%B1_...   \n",
       "34883         https://en.wikipedia.org/wiki/Olanlar_Oldu   \n",
       "34884  https://en.wikipedia.org/wiki/Non-Transferable...   \n",
       "34885  https://en.wikipedia.org/wiki/%C4%B0stanbul_K%...   \n",
       "\n",
       "                                                    Plot  \n",
       "0      A bartender is working at a saloon, serving dr...  \n",
       "1      The moon, painted with a smiling face hangs ov...  \n",
       "2      The film, just over a minute long, is composed...  \n",
       "3      Lasting just 61 seconds and consisting of two ...  \n",
       "4      The earliest known adaptation of the classic f...  \n",
       "...                                                  ...  \n",
       "34881  The film begins in 1919, just after World War ...  \n",
       "34882  Two musicians, Salih and Gürkan, described the...  \n",
       "34883  Zafer, a sailor living with his mother Döndü i...  \n",
       "34884  The film centres around a young woman named Am...  \n",
       "34885  The writer Orhan Şahin returns to İstanbul aft...  \n",
       "\n",
       "[34886 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar columnas de interés\n",
    "documents = df['Plot'].tolist()  # Textos principales de las películas\n",
    "ids = df.index.tolist()  # Usamos los índices como IDs únicos\n",
    "metadatas = df[['Title']].to_dict(orient='records')  # Metadatos relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Generar embeddings**\n",
    "\n",
    "Usamos SentenceTransformer para convertir cada documento en un vector numérico (embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la cantidad de documentos a procesar por motivos de recursos disponibles\n",
    "num_docs = 50  \n",
    "\n",
    "# Generar los embeddings solo para los primeros `num_docs` documentos\n",
    "embeddings = model.encode(documents[:num_docs]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Configurar e inicializar ChromaDB**\n",
    "\n",
    "Inicializamos el cliente de ChromaDB para crear o cargar la colección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Inicializar la base de datos vectorial con persistencia\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")  # Almacenará los datos en disco\n",
    "\n",
    "# Crear o cargar la colección\n",
    "collection = client.get_or_create_collection(\"wiki_movies_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Agregar los documentos a la colección**\n",
    "\n",
    "Añadimos los textos (documents), sus metadatos, IDs y embeddings a la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos: 50\n",
      "Metadatos: 50\n",
      "IDs: 50\n",
      "Embeddings: 50\n"
     ]
    }
   ],
   "source": [
    "# Asegurar que todas las listas tengan la misma longitud\n",
    "documents = documents[:num_docs]\n",
    "metadatas = metadatas[:num_docs]\n",
    "ids = ids[:num_docs]\n",
    "\n",
    "# Verificar longitudes\n",
    "print(f\"Documentos: {len(documents)}\")\n",
    "print(f\"Metadatos: {len(metadatas)}\")\n",
    "print(f\"IDs: {len(ids)}\")\n",
    "print(f\"Embeddings: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos agregados exitosamente a la colección.\n"
     ]
    }
   ],
   "source": [
    "# Convertir todos los IDs a cadenas de texto\n",
    "ids = [str(id) for id in ids]\n",
    "\n",
    "# Agregar los documentos, metadatos, IDs y embeddings a la colección\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(\"Documentos agregados exitosamente a la colección.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6: Realizar la consulta**\n",
    "\n",
    "Convertimos la consulta \"man\" en un embedding y buscamos los documentos más similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la consulta:\n",
      "Resultado 1:\n",
      "[\"A thug accosts a girl as she leaves her workplace but a man rescues her. The thug vows revenge and, with the help of two friends, attacks the girl and her rescuer again as they're going for a walk. This time they succeed in kidnapping the rescuer. He is bound and gagged and taken away in a cart. The girl runs home and gets help from several neighbors. They track the ruffians down to a cabin in the mountains where the gang has trapped their victim and set the cabin on fire. A thug and Rescuer fight on the roof of the house.\", 'Only lasting 15 minutes, it is a light-hearted comedy about the battle between the sexes as several married couples go on a camp-out together. The women soon realize that the men expect them to do perform all of the work while they relax, leading to several comedic situations.', \"Before heading out to a baseball game at a nearby ballpark, sports fan Mr. Brown drinks several highball cocktails. He arrives at the ballpark to watch the game, but has become so inebriated that the game appears to him in reverse, with the players running the bases backwards and the baseball flying back into the pitcher's hand. After the game is over, Mr. Brown is escorted home by one of his friends. When they arrive at Brown's house, they encounter his wife who becomes furious with the friend and proceeds to physically assault him, believing he is responsible for her husband's severe intoxication.[1]\"]\n",
      "Metadatos: [{'Title': 'The Black Viper'}, {'Title': 'Petticoat Camp'}, {'Title': 'How Brown Saw the Baseball Game'}]\n"
     ]
    }
   ],
   "source": [
    "# Consulta\n",
    "query = \"man\"\n",
    "\n",
    "# Generar el embedding de la consulta\n",
    "query_embedding = model.encode([query]).tolist()\n",
    "\n",
    "# Realizar la consulta\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3  # Devuelve los 3 documentos más similares\n",
    ")\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Resultados de la consulta:\")\n",
    "for i, result in enumerate(results['documents']):\n",
    "    print(f\"Resultado {i+1}:\")\n",
    "    print(result)\n",
    "    print(\"Metadatos:\", results['metadatas'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
